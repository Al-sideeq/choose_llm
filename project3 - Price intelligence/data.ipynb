{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46d90d45-2d19-49c7-b853-6809dc417ea7",
   "metadata": {},
   "source": [
    "## Dataset Curator Amazon Appliance project\n",
    "\n",
    "This has been moved to a Google Colab:\n",
    "https://colab.research.google.com/drive/1_7UXT4dp2Cr0RiRzMkMFxKgiGSt5QpVU#scrollTo=MDyR63OTNUJ6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16cf3aa2-f407-4b95-8b9e-c3c586f67835",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import re\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from datasets import Dataset\n",
    "from dotenv import load_dotenv\n",
    "from huggingface_hub import login\n",
    "import transformers\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd01f65-a5db-4431-add0-b140d59d355a",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_MODEL = \"meta-llama/Meta-Llama-3.1-8B\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "375302b6-b6a7-46ea-a74c-c2400dbd8bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables in a file called .env\n",
    "load_dotenv()\n",
    "os.environ['HF_TOKEN'] = os.getenv('HF_TOKEN', 'your-hf-token-if-not-using-env')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a43580d5-e70d-4952-8056-fb07ab6f56d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"McAuley-Lab/Amazon-Reviews-2023\", \"raw_meta_Appliances\", split=\"full\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "925d8684-e286-4e00-9097-866256d6e15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset[0].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e0209dd-3c73-41e8-8197-b0a081921f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "item = dataset[0]\n",
    "print(item['title'])\n",
    "print(item['description'])\n",
    "print(item['features'])\n",
    "print(item['price'])\n",
    "print(item['price']==\"None\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c80947e3-b120-4118-baf6-fca343143487",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Item:\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, trust_remote_code=True)\n",
    "    stop = set(['the', 'and', 'for', 'is', 'to', 'this', 'with', 'a', 'of', 'your', 'are', 'in','from', 'you', 'or', 'an'])\n",
    "\n",
    "    \n",
    "    def __init__(self, data):\n",
    "        self.title = data['title']\n",
    "        self.description = data['description']\n",
    "        self.features = data['features']\n",
    "        self.price = data['price']\n",
    "        self._token_count = None\n",
    "\n",
    "    def inference_prompt(self):\n",
    "        prompt = \"Predict the price of this item.\\n\"\n",
    "        prompt += f\"Title: {self.title}\\n\"\n",
    "        prompt += f\"Description: {self.description}\\n\"\n",
    "        prompt += f\"Features: {self.features}\\n\"\n",
    "        prompt += f\"The answer: Predicted price=$\"\n",
    "        return prompt\n",
    "\n",
    "    def train_prompt(self):\n",
    "        return f\"{self.inference_prompt()}{self.price}\"\n",
    "\n",
    "    def token_count(self):\n",
    "        if self._token_count == None:\n",
    "            self._token_count = len(self.tokenizer.encode(self.train_prompt()))\n",
    "        return self._token_count\n",
    "\n",
    "    def tokens_between(self, low, high):\n",
    "        token_count = self.token_count()\n",
    "        return token_count >= low and token_count < high\n",
    "\n",
    "    def words(self):\n",
    "        text = f\"{self.title} {self.description} {self.features}\"\n",
    "        text = re.sub(r'[()\\[\\]{},\\'\"-]', ' ', text)\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        words = text.strip().lower().split(' ')\n",
    "        return [word for word in words if word not in self.stop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecbb43d1-1cc6-4b10-ae05-39e19bcc471a",
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "for data in tqdm(dataset):\n",
    "    try:\n",
    "        price = float(data['price'])\n",
    "        if price>0:\n",
    "            items.append(Item(data))\n",
    "    except ValueError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25977c55-f16c-4782-9981-5ff53d511f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"There are {len(items):,} out of {len(dataset):,} with prices\")\n",
    "print(f\"\\nItem 0 has {items[0].token_count()} tokens:\")\n",
    "print(items[0].train_prompt())\n",
    "print(f\"\\nItem 1 has {items[1].token_count()} tokens:\")\n",
    "print(items[1].train_prompt())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b8c5fa5-9be0-42a5-83ec-85125cab85b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptions, features, both = 0, 0, 0\n",
    "for item in items:\n",
    "    description = str(item.description)\n",
    "    feature = str(item.features)\n",
    "    if len(description)>8: descriptions += 1\n",
    "    if len(feature)>8: features += 1\n",
    "    if len(description)>8 and len(feature)>8: both +=1\n",
    "print(len(items))\n",
    "print(descriptions, features, both)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a256451c-92ce-4dd9-aa57-f67b8ce22ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_counts = [item.token_count() for item in tqdm(items)]\n",
    "%matplotlib inline\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "ax.set_xlabel('Number of tokens')\n",
    "ax.set_ylabel('Count of items');\n",
    "_ = ax.hist(token_counts, rwidth=0.7, color=\"orange\", bins=range(0, 2000, 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ff036a-a486-4dcf-a709-43fd58c6f7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "low_cutoff = 100\n",
    "high_cutoff = 300\n",
    "subset = [item for item in tqdm(items) if item.tokens_between(low_cutoff, high_cutoff)]\n",
    "subset_count = len(subset)\n",
    "count = len(items)\n",
    "print(f\"Between {low_cutoff} and {high_cutoff}, we get {subset_count:,} out of {count:,} which is {subset_count/count*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "446bf3e1-b4b9-4596-8a3b-211667ef841f",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_counts = [item.token_count() for item in subset]\n",
    "%matplotlib inline\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "ax.set_xlabel('Number of tokens')\n",
    "ax.set_ylabel('Count of items');\n",
    "_ = ax.hist(token_counts, rwidth=0.7, color=\"purple\", bins=range(0, 400, 20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc22b66b-a7fe-4c89-ad13-4de9850422f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "random.shuffle(subset)\n",
    "split_index = int(len(subset) * 0.95)\n",
    "train = subset[:split_index]\n",
    "test = subset[split_index:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c581df67-fe64-4342-a9ab-c1862e04f33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "words = Counter()\n",
    "for item in train:\n",
    "    words.update(item.words())\n",
    "top_20_words = words.most_common(20)\n",
    "print(\"Top 20 words:\", top_20_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82fccf1c-8b9d-4de1-9eb9-5ae5a3a79a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.svm import SVR\n",
    "import numpy as np\n",
    "\n",
    "documents = [\" \".join(item.words()) for item in train]\n",
    "labels = np.array([float(item.price) for item in train])\n",
    "\n",
    "# vectorizer = CountVectorizer()\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(documents)\n",
    "\n",
    "# regressor = LinearRegression()\n",
    "regressor = SVR(kernel='linear')\n",
    "regressor.fit(X, labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f806956-4231-481a-bb68-892d753eaf2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "error_total = 0\n",
    "squared_log_error_total = 0\n",
    "size = 50\n",
    "for item in test[:size]:\n",
    "    x = vectorizer.transform([\" \".join(item.words())])\n",
    "    truth = float(item.price)\n",
    "    prediction = max(regressor.predict(x)[0], 0)\n",
    "    error_total += abs(truth - prediction)\n",
    "    squared_log_error_total += (math.log(truth+1) - math.log(prediction+1)) ** 2\n",
    "    print(f\"Result: Truth={truth} Prediction={prediction}\")\n",
    "error = error_total / size\n",
    "rmsle = math.sqrt(squared_log_error_total / size)\n",
    "print(f'Average error: {error:.2f}')\n",
    "print(f'Root mean squared log error: {rmsle:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f97135d9-628f-4f2c-af5e-2ab98b86912e",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY', 'your-key-if-not-using-env')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b24323-5c46-484a-8934-761b06ece439",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Frontier:\n",
    "    model = 'gpt-4o-mini'\n",
    "    gpt = OpenAI()\n",
    "    system_message = \"You predict product prices\"\n",
    "\n",
    "    def __init__(self, item):\n",
    "        self.guess = 0\n",
    "        self.item = item\n",
    "        self.truth = float(item.price)\n",
    "\n",
    "    @staticmethod\n",
    "    def extract_float_from_string(s):\n",
    "        match = re.search(r\"[-+]?\\d*\\.\\d+|\\d+\", s)\n",
    "        return float(match.group()) if match else 0\n",
    "\n",
    "    def run(self):\n",
    "        user_prompt = item.inference_prompt()\n",
    "        prompts = [\n",
    "            {\"role\": \"system\", \"content\": self.system_message},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ]\n",
    "        completion = self.gpt.chat.completions.create(\n",
    "            model='gpt-4o-mini',\n",
    "            messages=prompts,\n",
    "            max_tokens=8\n",
    "        )\n",
    "        response = completion.choices[0].message.content\n",
    "        self.guess = self.extract_float_from_string(response)\n",
    "\n",
    "    def error(self):\n",
    "        return abs(self.truth - self.guess)\n",
    "\n",
    "    def squared_log_error(self):\n",
    "        log_error = math.log(self.truth+1) - math.log(self.guess+1)\n",
    "        return log_error ** 2\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Truth={self.truth} Guess={self.guess} Error={self.error():.2f} SLE={self.squared_log_error():.2f}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8aeb674-58dd-4ab5-b5f8-7a4b22f3c002",
   "metadata": {},
   "outputs": [],
   "source": [
    "error_total = 0\n",
    "squared_log_error_total = 0\n",
    "size = 50\n",
    "for item in test[:size]:\n",
    "    frontier=Frontier(item)\n",
    "    frontier.run()\n",
    "    error_total += frontier.error()\n",
    "    squared_log_error_total += frontier.squared_log_error()\n",
    "    print(frontier)\n",
    "error = error_total / size\n",
    "rmsle = math.sqrt(squared_log_error_total / size)\n",
    "print(f'Average error: {error:.2f}')\n",
    "print(f'Root mean squared log error: {rmsle:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "758f028e-455c-4ff6-8313-30a60bbea42e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_prompts = [item.train_prompt() for item in train]\n",
    "train_prices = [float(item.price) for item in train]\n",
    "test_prompts = [item.inference_prompt() for item in test]\n",
    "test_prices = [float(item.price) for item in test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc49b7ca-4599-4054-8b7c-1595704abc1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Dataset from the lists\n",
    "train_dataset = Dataset.from_dict({\"text\": train_prompts, \"price\": train_prices})\n",
    "test_dataset = Dataset.from_dict({\"text\": test_prompts, \"price\": test_prices})\n",
    "\n",
    "# Combine the datasets into a DatasetDict for easy access\n",
    "dataset = DatasetDict({\n",
    "    \"train\": train_dataset,\n",
    "    \"test\": test_dataset\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06462d59-0210-4420-ba3b-5d7080732b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_NAME = \"ed-donner/appliances\"\n",
    "login(token=os.environ['HF_TOKEN'])\n",
    "dataset.push_to_hub(DATASET_NAME, private=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4691a025-9800-4e97-a20f-a65f102401f1",
   "metadata": {},
   "source": [
    "## And now to head over to a Google Colab for fine-tuning in the cloud\n",
    "\n",
    "Follow this link for the Colab: https://colab.research.google.com/drive/19E9hoAzWKvn9c9SHqM4Xan_Ph4wNewHS?usp=sharing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a6c3e0-a2e6-4115-a01a-45e79dfdb730",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "models = [\"Traditional\", \"GPT-4o-mini\", \"Llama 3.1 base\", \"Llama 3.1 finetuned\"]\n",
    "rmsle = [2.02, 1.15, 1.60, 0.56]\n",
    "\n",
    "# Convert RMSLE to accuracy-like metric\n",
    "accuracy = [1 / (e + 1) for e in rmsle]\n",
    "\n",
    "# Create a bar chart\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(models, rmsle, color=\"lightblue\", width=0.6)\n",
    "\n",
    "# Add titles and labels\n",
    "plt.title('Model Error')\n",
    "plt.xlabel('Models')\n",
    "plt.ylabel('Error (RMSLE)')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "520665db-705c-4708-bbb3-9eccfba9cacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "models = [\"Traditional ML\", \"GPT-4o\", \"Claude-3.5-Sonnet\", \"Llama 3.1 base\", \"Llama 3.1 finetuned\"]\n",
    "errors = [67.57, 63.12, 58.49, 83.44, 23.95]\n",
    "\n",
    "# Create a bar chart\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(models, errors, color=\"lightblue\", width=0.6)\n",
    "\n",
    "# Add titles and labels\n",
    "plt.title('Average Price Difference between Prediction and Actual across 50 Appliances')\n",
    "plt.xlabel('Models')\n",
    "plt.ylabel('Average Difference ($)')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7edf93-7abd-4f1b-b69f-d7cc065acc22",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
