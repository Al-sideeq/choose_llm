{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ef4de33-ff7a-4d59-bc0f-dee72df6c6d1",
   "metadata": {},
   "source": [
    "# Another way to call LLMs - directly on your box using Llama.cpp\n",
    "\n",
    "This notebook runs inference for models directly on your local box, using the open source C++ library llama.cpp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fcd7d28-148c-47b2-ad1b-35ce28557bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, install the llama-cpp library. Do some googling if you have problems with this install, or contact me\n",
    "\n",
    "!pip install llama-cpp-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b293860-cf1e-4c16-9bd3-852e62e9a402",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If any problems with this import, some investigating may be required..\n",
    "\n",
    "from llama_cpp import Llama"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49024a57-824c-40b4-8592-129dcb8ff116",
   "metadata": {},
   "source": [
    "## First, download a model locally\n",
    "\n",
    "The llama.cpp library uses its own model format called GGUF.\n",
    "\n",
    "Here are all the HuggingFace models that can be downloaded as a GGUF file:\n",
    "https://huggingface.co/models?library=gguf\n",
    "\n",
    "For this notebook, I downloaded 3 models to try. For each of these models, click download, and move the file from your downloads folder into the `model_cache` folder in this directory (which is .gitignored).\n",
    "\n",
    "First, this medium sized version of Microsoft's Phi-3:\n",
    "https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-gguf/blob/main/Phi-3-mini-4k-instruct-q4.gguf\n",
    "\n",
    "Then, this version of Qwen2:\n",
    "https://huggingface.co/Qwen/Qwen2-7B-Instruct-GGUF/blob/main/qwen2-7b-instruct-q4_k_m.gguf\n",
    "\n",
    "Finally, I chose the medium sized version of StarCoder2 for some coding inference.\n",
    "https://huggingface.co/second-state/StarCoder2-3B-GGUF/blob/main/starcoder2-3b-Q4_K_M.gguf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf2f7d54-ea6c-4572-8bd5-3b5b022c93e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is where my GGUF files are located\n",
    "\n",
    "phi3_model_path = \"model_cache/Phi-3-mini-4k-instruct-q4.gguf\"\n",
    "qwen2_model_path = \"model_cache/qwen2-7b-instruct-q4_k_m.gguf\"\n",
    "starcoder2_model_path = \"model_cache/starcoder2-3b-Q4_K_M.gguf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d981d8-42dd-4e1d-afa0-b598b2d24185",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now use llama.cpp to create the models for Phi 3, Qwen2 and Starcoder2\n",
    "\n",
    "phi3 = Llama(model_path=phi3_model_path, n_ctx=300)\n",
    "qwen2 = Llama(model_path=qwen2_model_path, n_ctx=300)\n",
    "starcoder2 = Llama(model_path=starcoder2_model_path, n_ctx=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "599ecb28-804d-4807-b0a9-5d15e1e1a430",
   "metadata": {},
   "outputs": [],
   "source": [
    "# phi3 tell us a joke! Remember this is a tiny model!\n",
    "# The prompt has some special characters in it - we'll cover this shortly\n",
    "\n",
    "prompt = \"\"\"<|user|>\n",
    "Tell a light joke for a room full of data scientists<|end|>\n",
    "<|assistant|>\"\"\"\n",
    "\n",
    "response = phi3(prompt, max_tokens=200, temperature=1, echo=True, stream=True)\n",
    "for chunk in response:\n",
    "    print(chunk[\"choices\"][0][\"text\"], end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128e5cc7-76a1-4056-a3d0-7d5ba1b5077d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# qwen2 tell us a joke!\n",
    "\n",
    "prompt = \"\"\"<|im_start|>system\n",
    "You are a helpful assistant.<|im_end|>\n",
    "<|im_start|>user\n",
    "Tell a light joke for a room full of data scientists<|im_end|>\n",
    "<|im_start|>assistant\"\"\"\n",
    "\n",
    "response = qwen2(prompt, max_tokens=200, temperature=1, echo=True, stream=True)\n",
    "for chunk in response:\n",
    "    print(chunk[\"choices\"][0][\"text\"], end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adca9c0b-4b4e-4457-9f48-03ea43a1755e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OK enough with the jokes - starcoder2 please write a function for us\n",
    "\n",
    "prompt = \"def hello_world():\\n\"\n",
    "response = starcoder2(prompt, max_tokens=100, temperature=1, echo=True, stream=True)\n",
    "for chunk in response:\n",
    "    print(chunk[\"choices\"][0][\"text\"], end='')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb62a609-e1d0-4395-a89a-f3ab5ce89761",
   "metadata": {},
   "source": [
    "## Finally: to try the other approach for direct inference: on a cloud box with a GPU\n",
    "### Using Hugging Face Hub and Transformers library\n",
    "\n",
    "Visit this Google Colab notebook:\n",
    "https://colab.research.google.com/drive/1CRgX6RVqnWZDexXLACbq91pX2I7O7Swu?usp=sharing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37bdf1c-7314-4bbb-9939-155d9cb85bb6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
